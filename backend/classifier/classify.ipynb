{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_' + metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_' + metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir = '..\\\\data'\n",
    "train_dir = f\"{dataset_dir}\\\\tr\"\n",
    "batch_size = 32\n",
    "time = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeler(example, index):\n",
    "    return example, tf.cast(index, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAMES = ['combinedneg.txt', 'combinedneut2.txt', 'combinedpos.txt']\n",
    "labeled_data_sets = []\n",
    "# Give labels to the data, 0 for negatives, 1 for neutrals, 2 for positives.\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "    lines_dataset = tf.data.TextLineDataset(f\"{train_dir}\\\\{file_name}\")\n",
    "    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "    labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all labeled datasets into one and shuffle\n",
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "    all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = [i for i,_ in enumerate(all_labeled_data)][-1] + 1\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SIZE = math.ceil(DATASET_SIZE * 0.1)\n",
    "VOCAB_SIZE = 45000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Labels are 0 = negative, 1 = neutral, 2 = positive.\\n')\n",
    "for text, label in all_labeled_data.take(1):\n",
    "    print(\"Sentence: \", text.numpy())\n",
    "    print(\"Label:\", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = all_labeled_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = all_labeled_data.take(VALIDATION_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print('texts: ', example.numpy()[:3])\n",
    "  print()\n",
    "  print('labels: ', label.numpy()[:3])\n",
    "encoder = TextVectorization()\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "print(len(vocab), vocab[:20])\n",
    "encoded_example = encoder(example)[:3].numpy()\n",
    "print(encoded_example)\n",
    "for n in range(3):\n",
    "    print(\"Original: \", example[n].numpy())\n",
    "    print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"..\\\\data\\\\others\\\\vocab2.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(vocab, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_labels):\n",
    "    model_layers = tf.keras.Sequential([\n",
    "        encoder,\n",
    "        layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=64, embeddings_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4), mask_zero=True),\n",
    "        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "        layers.Bidirectional(layers.LSTM(32)),\n",
    "        layers.Dropout(0.20),\n",
    "        layers.Dense(32, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4), activation='relu'),\n",
    "        layers.Dropout(0.20),\n",
    "        layers.Dense(num_labels),\n",
    "        layers.Activation('softmax')])\n",
    "    return model_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(num_labels=3)\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(1e-4),\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits = False),\n",
    "    metrics=['accuracy'])\n",
    "history = model.fit(train_dataset, validation_data=test_dataset, callbacks=[early_stop], epochs=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'model/rnn_{time}')\n",
    "# model.save(f'model/rnn_{time}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = f\"model/rnn_{time}\"\n",
    "import_model = tf.keras.models.load_model(modelpath)\n",
    "import_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "inputs = []\n",
    "with open(f\"D:\\\\Work\\\\Data\\\\s24_2001_sentences_shuffled_slice.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    random.shuffle(lines)\n",
    "    for line in lines:\n",
    "        inputs.append(line.strip('\\n'))\n",
    "inputs = np.array(inputs[:n])\n",
    "print(inputs[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_prediction(inps):\n",
    "    predicted_scores = import_model.predict(np.array(inps))\n",
    "    predicted_labels = tf.argmax(predicted_scores, axis=1)\n",
    "    lab, pred = '', ''\n",
    "    labs, preds = [], []\n",
    "    for i, (inp, label) in enumerate(zip(inputs, predicted_labels)):\n",
    "        pred = (', '.join(f'{q:.5f}' for q in predicted_scores[i]))\n",
    "        lab = label.numpy()\n",
    "        print(f\"Sentence: {inp}\")\n",
    "        print(f\"Predicted label: {lab}\")\n",
    "        print(f\"Predicted probs: {pred}\\n\")\n",
    "        labs.append(lab)\n",
    "        preds.append(pred)\n",
    "    return labs, preds\n",
    "\n",
    "print(np.array(inputs), '\\n\\n', make_prediction(inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
