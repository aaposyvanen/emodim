{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26abb37e-59a5-4832-91c6-49502b8933d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "time = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TurkuNLP/bert-base-finnish-cased-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03582e05-9063-4e27-b110-de3810625fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ei mitään muttia.', 'älä anna hänen määräillä sinua!', 'laske aseet maahan.', 'maksut pitää hoitaa ajallaan.', 'lntiaanirenki, juoppo pyssymies, seksihullu ja eno!'] [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "texts, labels, files = [], [], [\"combinedneg\", \"combinedneut2\", \"combinedpos\"]\n",
    "for i, file in enumerate(files):\n",
    "    with open(f\"E:/Emodim/data/tr/{file}.txt\", \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        texts.extend(lines), labels.extend([i]*len(lines))\n",
    "print(texts[:5], labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2296c10b-f900-436b-9e6d-0e997b033b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=.1)\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c99b973-79d0-4576-9494-270988c5e70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at TurkuNLP/bert-base-finnish-cased-v1 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"E:\\Miniconda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"E:\\Miniconda\\envs\\tf\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 947, in compute_loss  *\n        return super().compute_loss(*args, **kwargs)\n    File \"E:\\Miniconda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss  **\n        return self.compiled_loss(\n    File \"E:\\Miniconda\\envs\\tf\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 199, in __call__\n        y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\n    File \"E:\\Miniconda\\envs\\tf\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 726, in match_dtype_and_rank\n        if ((y_t.dtype.is_floating and y_p.dtype.is_floating) or\n\n    AttributeError: 'NoneType' object has no attribute 'dtype'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompute_loss) \u001b[38;5;66;03m# can also use any keras loss fn\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Miniconda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filezmpr1of1.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Miniconda\\envs\\tf\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1000\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    998\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_loss(y_pred\u001b[38;5;241m.\u001b[39mloss, y_pred\u001b[38;5;241m.\u001b[39mloss, sample_weight, regularization_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1000\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_loss(y, y_pred, sample_weight, regularization_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;66;03m# Run backwards pass.\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mminimize(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables, tape\u001b[38;5;241m=\u001b[39mtape)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filepjiqldm7.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m         do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mhasattr\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompute_loss\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_return\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretval_\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fscope\u001b[38;5;241m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filepjiqldm7.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss.<locals>.if_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28msuper\u001b[39m), (), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\u001b[38;5;241m.\u001b[39mcompute_loss, \u001b[38;5;28mtuple\u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(args)), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(kwargs)), fscope)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"E:\\Miniconda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"E:\\Miniconda\\envs\\tf\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 947, in compute_loss  *\n        return super().compute_loss(*args, **kwargs)\n    File \"E:\\Miniconda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss  **\n        return self.compiled_loss(\n    File \"E:\\Miniconda\\envs\\tf\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 199, in __call__\n        y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\n    File \"E:\\Miniconda\\envs\\tf\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 726, in match_dtype_and_rank\n        if ((y_t.dtype.is_floating and y_p.dtype.is_floating) or\n\n    AttributeError: 'NoneType' object has no attribute 'dtype'\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"TurkuNLP/bert-base-finnish-cased-v1\")\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
    "\n",
    "\"\"\"\n",
    "model.compile(\n",
    "              optimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=5e-5, weight_decay=0.01),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "              metrics = tf.keras.metrics.SparseCategoricalAccuracy())\n",
    "history = model.fit(train_dataset, batch_size=16, callbacks=[early_stop], epochs=10)\n",
    "\"\"\"\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False)) # can also use any keras loss fn\n",
    "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288934a-3afd-4654-a328-43614538c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = f'E:\\Emodim\\data\\others\\\\fine_tuned_finBERT_tf_{time}'\n",
    "model.save(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0fda84-729c-4305-828a-f2a3434e6024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_model = tf.keras.models.load_model(modelpath)\n",
    "import_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9faeb9-8178-44c7-80ca-46644388709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "n = 50\n",
    "inputs = []\n",
    "with open(f\"D:\\\\Work\\\\Data\\\\s24_2001_sentences_shuffled_slice.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    random.shuffle(lines)\n",
    "    for line in lines:\n",
    "        inputs.append(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d29cd2-100e-41bb-975a-1abef3f2e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for inp in inputs[:n]:\n",
    "    input_text_tokenized = tokenizer.encode(inp, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "    prediction = model(input_text_tokenized)\n",
    "    prediction_logits = prediction[0]\n",
    "    prediction_probs = tf.nn.softmax(prediction_logits,axis=1).numpy()\n",
    "    print(f'Sentence: {inp}, Probabilities: {prediction_probs}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
